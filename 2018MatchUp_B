by prof. 이승철(포항공과대학교 기계공학과)

MatchUp Series
a. 머신러닝을 위한 파이썬 기초
b. 머신러닝을 위한 선형대수와 최적화
c. 지도학습: 회귀
d. 지도학습: 분류
e. 비지도 학습: 군집화와 차원축소


b. 머신러닝을 위한 선형대수와 최적화

머신러닝을 위한 선형대수 이해 및 활용(1)

Linear Algebra 선형대수
- 행렬, 벡터, 내적, 벡터크기(norm)
- 선형변환, 고유치 문제
- 투영변환, 최소자승법

Optimization 최적화
- convex optimization -> CVXPY module
- 경사 하강법 gradient descent

1. 선형 연립 방정식
- 연립 방정식을 역행렬을 이용해 표현할 수 있음(역행렬이 존재하는 경우)
- 차원이 확장되어도(고차원) 간결한 모양으로 나타낼 수 있음

2. 행렬/벡터 곱
- vector: 방향과 크기를 모두 포함. 표기는 문자 위 화살표(→) c.f., scalar: 크기만 의미
- 강의에서는 column vector로 표현 통일
- vector*vector: inner product 내적
  - 둘 다 column vector라면 한 행렬을 transpose
  - 각 scalar를 더한 형태가 됨
- column vector 열벡터 & row vector 행벡터
  - 2*2로 이루어진 일반 행렬도 row, column vector로 표현할 수 있음
- 행렬과 벡터의 곱
  - row vector: each entry of Ax is an inner product between x and a row of A
  - column vector: Ax is a linear combination of the columns of A, with coefficients given(x가 Ax의 weight에 대한 정보를 갖고 있음)

3. 벡터의 크기
- scalar는 크기. 원점에서의 거리 e.g.,|2| = 2, |-3| = 3
- norms: ||x|| measures length of vector from origin. vector의 크기
  - 거리를 나타내는 두 가지 정의가 있음
    - L2 norm: 유클리디안 공간에서 최단거리(대각선. 피타고라스 정리). n차원으로 확장된 거리 
    - L1 norm: 각각 거리를 모두 더한 것. e.g., 바둑판에서 a지점까지 가는 방법
- orthogonality 직교성
  - 두 벡터의 곱 xy = 0(x는 transpose)이라면 orthogonal. inner product가 0이기 때문
  - xy = 0이고, ||x|| = ||y|| = 1이라면(각각의 크기가 1) orthonormal
- 두 벡터 사이의 각도
  - xy = ||x|| cos𝜃 ||y||
    cos𝜃 = xy / ||x||*||y||
  - n 차원으로 확장해도 같은 방식으로 사이각𝜃를 구할 수 있음
  - 사이각을 안다는 것은,
    - xy = 0이라면(y given), x가 y에 직교하는 곳에 분포한다는 것(orthogonal)
    - xy <= 0이라면, y가 향하는 곳의 뒤쪽 부분(작은 쪽)에 분포
    - 즉, 공간을 분리할 수 있음-> half space
